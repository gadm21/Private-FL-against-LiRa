{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import * \n",
    "from Fed import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MyMIA : \n",
    "\n",
    "    def __init__(self, target_data, similar_data, target_model, target_model_fn, attack_model_fn, params) : \n",
    "        self.target_data = target_data\n",
    "        self.similar_data = similar_data\n",
    "        self.target_model = target_model\n",
    "        self.target_model_fn = target_model_fn\n",
    "        self.attack_model_fn = attack_model_fn\n",
    "        self.params = params\n",
    "\n",
    "        # We assume that attacker's data were not seen in target's training.\n",
    "        self.similar_x_in, self.similar_x_out, self.similar_y_in, self.similar_y_out = train_test_split(\n",
    "            similar_data[0], similar_data[1], test_size=0.1\n",
    "        )\n",
    "\n",
    "\n",
    "    def train_shadows(self) :\n",
    "        # Train the shadow models.\n",
    "        \n",
    "        self.smb = ShadowModelBundle(\n",
    "            self.target_model_fn, \n",
    "            shadow_dataset_size=len(self.similar_x_in) // self.params['num_shadows'],\n",
    "            num_models=self.params['num_shadows']\n",
    "        )\n",
    "        \n",
    "        print(\"Training the shadow models...\")\n",
    "        self.x_attack, self.y_attack = self.smb.fit_transform(\n",
    "            self.similar_x_in,\n",
    "            self.similar_y_in,\n",
    "            fit_kwargs=dict(\n",
    "                epochs=self.params['shadow_epochs'],\n",
    "                verbose=False,\n",
    "                validation_data=(self.similar_x_out, self.similar_y_out),\n",
    "            ),\n",
    "        )\n",
    "\n",
    "\n",
    "    def train_attacker(self) :\n",
    "        # ShadowModelBundle returns data in the format suitable for the AttackModelBundle.\n",
    "        \n",
    "        self.amb = AttackModelBundle(self.attack_model_fn, num_classes= self.params['num_classes'])\n",
    "\n",
    "        # Fit the attack models.\n",
    "        print(\"Training the attack models...\")\n",
    "        self.amb.fit(\n",
    "            self.x_attack, self.y_attack\n",
    "        )\n",
    "\n",
    "\n",
    "    def test_attack(self) : \n",
    "        # Prepare examples that were in the training, and out of the training.\n",
    "        data_in = self.target_data\n",
    "        data_out = self.similar_data\n",
    "\n",
    "        # Compile them into the expected format for the AttackModelBundle.\n",
    "        attack_test_data, real_membership_labels = prepare_attack_data(\n",
    "            self.target_model, data_in, data_out\n",
    "        )\n",
    "\n",
    "        attack_guesses = self.amb.predict(attack_test_data)\n",
    "        attack_tp = np.mean((attack_guesses == 1) == (real_membership_labels == 1))\n",
    "        self.attack_accuracy = np.mean(attack_guesses == real_membership_labels)\n",
    "\n",
    "\n",
    "        return self.attack_accuracy, classification_report(real_membership_labels, attack_guesses)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FEDERATED LEARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = argparse.Namespace(\n",
    "    dataset='cifar10',\n",
    "    learning_algorithm='fedakd',\n",
    "    num_clients=5,\n",
    "    local_size=5000,\n",
    "    rounds=20,\n",
    "    local_epochs=2,\n",
    "    batch_size=32,\n",
    "    proxy_data_size=5000,\n",
    "    early_stop_patience=10,\n",
    "    lr = 0.001,\n",
    "    lr_reduction_patience=10,\n",
    "    target_model='nn',\n",
    "    dp_epsilon=0.5,\n",
    "    dp_delta=1e-5,\n",
    "    use_dp=True,\n",
    "    dp_type = 'dp',\n",
    "    dp_norm_clip=1.0,\n",
    "    temperature=0.7,\n",
    "    aalpha=1000,\n",
    "    bbeta=1000,\n",
    ")\n",
    "\n",
    "# iterate over the parameters of args \n",
    "params = dict()\n",
    "for k, v in vars(args).items():\n",
    "    params[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "\n",
      "systemMemory: 8.00 GB\n",
      "maxCacheSize: 2.67 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-23 19:31:48.418062: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-06-23 19:31:48.418508: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "experiment_id = args.dataset + '_' + args.learning_algorithm + '_' + datetime.now().strftime(\"%d_%m_%Y_%H_%M_%S\")\n",
    "experiment_dir = os.path.join(RESULTS_PATH, experiment_id)\n",
    "if not os.path.exists(experiment_dir):\n",
    "    os.makedirs(experiment_dir)\n",
    "\n",
    "\n",
    "\n",
    "train_data, test_data, metadata = get_data(args.dataset)\n",
    "args.input_shape = metadata['input_shape']\n",
    "args.num_classes = metadata['num_classes']\n",
    "args.class_labels = metadata['class_labels']\n",
    "\n",
    "train_data = (np.array(train_data[0] / 255, dtype=np.float32), tf.keras.utils.to_categorical(train_data[1]))\n",
    "test_data = (np.array(test_data[0] / 255, dtype=np.float32), tf.keras.utils.to_categorical(test_data[1]))\n",
    "\n",
    "\n",
    "\n",
    "centralized_data, clients_data, external_data = split_data(train_data, args.num_clients, args.local_size)\n",
    "\n",
    "proxy_limit = args.proxy_data_size\n",
    "proxy_data, _, _ = get_data('cifar100')\n",
    "proxy_data = (np.array(proxy_data[0][:proxy_limit] / 255, dtype=np.float32), tf.keras.utils.to_categorical(proxy_data[1][:proxy_limit]))\n",
    "\n",
    "# exp_path, clients_data, test_data, initial_model, args\n",
    "# learning_algorithm = FedAKD(experiment_dir, clients_data, test_data, proxy_data, create_model_based_on_data, args)\n",
    "initial_model = create_model_based_on_data(args, compile_model = False)\n",
    "learning_algorithm = FedAvg(experiment_dir, clients_data, test_data, initial_model, args)\n",
    "# t_model = learning_algorithm.create_temperature_scaled_model(initial_model, args.temperature)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_aggregate(weights) : \n",
    "    weight_accumulator = None\n",
    "    for weight in weights :\n",
    "        if weight_accumulator is None:\n",
    "            # If accumulator is uninitialized, set it to the client weights\n",
    "            weight_accumulator = weight\n",
    "        else:\n",
    "            # Add the client weights to the accumulator\n",
    "            weight_accumulator = [\n",
    "                accumulator + client_weight\n",
    "                for accumulator, client_weight in zip(weight_accumulator, weights)\n",
    "        ]\n",
    "    weight_accumulator = [accumulator / len(clients_models) for accumulator in weight_accumulator]\n",
    "    return weight_accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [model.get_weights() for model in learning_algorithm.clients_models]\n",
    "\n",
    "\n",
    "# new_weights = np.empty_like(weights[0]) \n",
    "\n",
    "# for i in range(len(weights[0])) :\n",
    "#     for j in range(len(weights[0][i])) :\n",
    "#         new_weights[i][j] = np.sum([weight[i][j] for weight in weights])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "avg_weights =[] \n",
    "for layer_id in range(len(weights[0]) ): \n",
    "    avg_layer = np.mean([weights[i][layer_id] for i in range(len(weights))], axis = 0)\n",
    "    avg_weights.append(avg_layer)\n",
    "\n",
    "learning_algorithm.clients_models[0].set_weights(avg_weights)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Central MIA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "eepochs = np.arange(1, 50, 5)\n",
    "accs = [] \n",
    "for eep in eepochs :\n",
    "\n",
    "    args = argparse.Namespace(\n",
    "        dataset='cifar10',\n",
    "        learning_algorithm='central',\n",
    "        num_clients=5,\n",
    "        local_size=500,\n",
    "        rounds=30,\n",
    "        local_epochs=eep,\n",
    "        proxy_data_size=1000,\n",
    "        early_stop_patience=10,\n",
    "        lr_reduction_patience=10,\n",
    "        target_model='nn',\n",
    "        epsilon=0.5,\n",
    "        dp='dp'\n",
    "    )\n",
    "\n",
    "    experiment_id = args.dataset + '_' + args.learning_algorithm + '_' + datetime.now().strftime(\"%d_%m_%Y_%H_%M_%S\")\n",
    "    experiment_dir = os.path.join(RESULTS_PATH, experiment_id)\n",
    "    if not os.path.exists(experiment_dir):\n",
    "        os.makedirs(experiment_dir)\n",
    "\n",
    "\n",
    "    train_data, test_data, metadata = get_data(args.dataset)\n",
    "\n",
    "    train_data = (np.array(train_data[0] / 255, dtype=np.float32), tf.keras.utils.to_categorical(train_data[1]))\n",
    "    test_data = (np.array(test_data[0] / 255, dtype=np.float32), tf.keras.utils.to_categorical(test_data[1]))\n",
    "\n",
    "\n",
    "    centralized_model = create_model_based_on_data(metadata)\n",
    "    callbacks = {\n",
    "        'early_stop_patience' : args.early_stop_patience,\n",
    "        'lr_reduction_patience' : args.lr_reduction_patience,\n",
    "        'csv_logger_path' : join(experiment_dir, 'centralized.csv')\n",
    "    }\n",
    "    history = train_keras_model(centralized_model, train_data, test_data, epochs=args.local_epochs, verbose=0, **callbacks)\n",
    "\n",
    "\n",
    "\n",
    "    mia_params = {\n",
    "        'num_shadows': 3,  # 3 shadow models\n",
    "        'num_classes': 10, # number of output classes\n",
    "        'shadow_epochs': 10,\n",
    "        'attack_epochs': 10,\n",
    "        \n",
    "    }\n",
    "\n",
    "    mia = MyMIA(train_data, test_data, centralized_model, create_model_based_on_data, attack_model_fn, mia_params)\n",
    "\n",
    "    mia.train_shadows() \n",
    "\n",
    "    mia.train_attacker()\n",
    "\n",
    "    acc, clf_rep = mia.test_attack()\n",
    "    print(\"epoch : \", eep, \" acc : \", acc)\n",
    "    accs.append(acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(accs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
